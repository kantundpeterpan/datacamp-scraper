{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my last two posts ([here](/building-a-datacamp-archive/) and [here](/webscraping-datacamp-courses-ii/)), I discussed my efforts to scrape the [DataCamp](http://datacamp.com/) website so that I could have a record of the courses I've taken. In this final post, I'm putting all the pieces together, discussing how the script works and showing what the final product looks like. \n",
    "\n",
    "There's a lot of code here, so I've frontloaded it into the first half of the post. You can choose your own adventure. If you're interested in how the script works, go to the next section. If you're interested in seeing what the script does, skip to the section \"In Action.\"\n",
    "\n",
    "Before I dive in, I want to say how excited I am about this project. A year ago, I could barely print `\"Hello world\"`, but thanks to sites like [DataCamp](http://datacamp.com/), communities like [The Carpentries](https://carpentries.org) and a heavy dose of [Stack Overflow](https://stackoverflow.com), I'm doing things like this. So if you're interested in coding or data science but think its something you could never do, think again. Keep a growth mindset, and dive  in.\n",
    "\n",
    "# How It Works\n",
    "\n",
    "Here's how the script works. The user passes the link of the course they want to `get_whole_course()`. This function then uses `get_course_outline()` to get a list of the chapters and lessons in each course, `make_chapter_notes()` to create a text file for each chapter, and `download_chapter_slides()` to download a PDF of the chapter slides. It then creates a text file with a table of contents for each course. This allows for easy navegation of the course once all the text files are in my notes system (I use [nvALT](http://brettterpstra.com/projects/nvalt/) and [The Archive](http://zettelkasten.de)). Everything is organized using unqiue, 14-digit \"z numbers,\" which are assigned to each course and chapter.\n",
    "\n",
    "Here are the libraries I used, with a brief explanation of the role each one plays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\t\t\t\t\t#For parsing the HTML\n",
    "from collections import OrderedDict\t\t\t\t#For storing chapter and lesson info\n",
    "from subprocess import Popen, PIPE, STDOUT\t\t#For accessing pandoc\n",
    "from urllib.request import urlopen, urlretrieve\t#For fetching the webpage, downloading PDF of slides\n",
    "import json\t\t\t\t\t\t\t\t\t\t#For dealing with website data that comes in JSON\n",
    "import pandas as pd\t\t\t\t\t\t\t\t#For create Zettelkasten-style filenames\n",
    "import re\t\t\t\t\t\t\t\t\t\t#For regex parsing of `sct`\n",
    "import subprocess\t\t\t\t\t\t\t\t#For accessing pandoc\n",
    "import pprint\t\t\t\t\t\t\t\t\t#For troubleshooting, looking through webpages, dictionaries, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's all the code you need to capture an entire course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_whole_course(link, return_z=False):\n",
    "\t'''Receives course URL from user. Gets ordered dict of chapters/lessons. \n",
    "\tCreates list of unique z_numbers, one for each chapter. \n",
    "\tCreates txt file for each chapter, fills each file with lesson content.\n",
    "\tGets course name. Creates table of contents.\n",
    "\tFeeds to: get_course_outline(), make_z_number(), make_chapter_notes(), \n",
    "\tget_course_title(), create_table_of_contents(), download_chapter_slides()'''\n",
    "\tcourse_dictionary = get_course_outline(link)\n",
    "\tz_list = make_z_number(len(course_dictionary.items())+1)\n",
    "\tchapter_names_Z_list = []\n",
    "\tfor chapter, lesson in course_dictionary.items():\n",
    "\t\tz_index = list(course_dictionary.keys()).index(chapter)\n",
    "\t\tfilename = z_list[z_index] + ' ' + chapter + '.txt'\n",
    "\t\tchapter_names_Z_list.append(filename)\n",
    "\t\tchapter_link = [lesson][0][1][1]\n",
    "\t\tmake_chapter_notes(filename, chapter_link)\n",
    "\tcourse_name = get_course_title(link)\n",
    "\tcreate_table_of_contents(course_dictionary, course_name, z_list)\n",
    "\tfirst_key = list(course_dictionary.keys())[0]\n",
    "\tone_link = course_dictionary[first_key][0][1]\n",
    "\tdownload_chapter_slides(chapter_names_Z_list, one_link)\n",
    "\tif return_z == True:\n",
    "\t\treturn z_list[-1]\n",
    "    \n",
    "def get_course_outline(link):\n",
    "\t'''Receives link to course landing page from get_whole_course(). \n",
    "\tReturns ordered dict of chapters with lessons.'''\n",
    "\thtml = urlopen(link)\n",
    "\tsoup = BeautifulSoup(html, 'lxml')\n",
    "\tlesson_outline = soup.find_all(['h4', 'li'])\n",
    "\tchapters = OrderedDict()   # {chapter: [(lesson_name, lesson_link), ...], ...}\n",
    "\tfor item in lesson_outline:\n",
    "\t\tattributes = item.attrs\n",
    "\t\ttry:\n",
    "\t\t\tclass_type = attributes['class'][0]\n",
    "\t\t\tif class_type == 'chapter__title':\n",
    "\t\t\t\tchapter = item.text.strip()\n",
    "\t\t\t\tchapters[chapter] = []\n",
    "\t\t\tif class_type == 'chapter__exercise':\n",
    "\t\t\t\tlesson_name = item.find('h5').text\n",
    "\t\t\t\tlesson_link = item.find('a').attrs['href']\n",
    "\t\t\t\tchapters[chapter].append((lesson_name, lesson_link))\n",
    "\t\texcept KeyError:\n",
    "\t\t\tpass\n",
    "\treturn(chapters)\n",
    "\n",
    "def make_z_number(num):\n",
    "\t'''Takes int and returns a list of unique, 14-digit numbers. Only goes to 99.'''\n",
    "\tassert num < 100, 'Enter an int that is less than 100. Max list size is 99.'\n",
    "\tstring_list = []\n",
    "\tz_index = 0\n",
    "\tfor x in range(num):\n",
    "\t\tz_string = pd.to_datetime('now').strftime('%Y%m%d%H%S') #Did seconds instead of minutes to avoid repeats\n",
    "\t\tz_string = z_string + '{0:0>2}'.format(z_index)\n",
    "\t\tstring_list.append(z_string)\n",
    "\t\tz_index += 1\n",
    "\treturn string_list\n",
    "\n",
    "def make_chapter_notes(filename, link):\n",
    "\t'''Receives filename and lesson link from get_whole_course().\n",
    "\t(Note that a link from any lesson in a chapter will work. \n",
    "    That is, any lesson link has all the information for the chapter.)\n",
    "\tCycles through all lessons in chapter, converting each lesson and sub-exercise from HTML to Markdown.\n",
    "\tPrints all chapter content into text file.\n",
    "\tFeeds to: get_lesson_json(), NormalExercise_print(), BulletExercise_print(), \n",
    "\tMultipleChoiceExercise_print(), download_chapter_slides()'''\n",
    "\tlesson_json = get_lesson_json(link)\n",
    "\tfor item in lesson_json['exercises']['all']:\n",
    "\t\tif item['type'] == 'VideoExercise':\n",
    "\t\t\tpass\n",
    "\t\telif item['type'] == 'NormalExercise':\n",
    "\t\t\tNormalExercise_print(item, filename)\n",
    "\t\telif item['type'] == 'BulletExercise':\n",
    "\t\t\tBulletExercise_print(item, filename)\n",
    "\t\telif item['type'] == 'MultipleChoiceExercise':\n",
    "\t\t\tMultipleChoiceExercise_print(item, filename)\n",
    "\n",
    "def get_course_title(link):\n",
    "\t'''Receives link from get_whole_course(). Gets website. Returns title.'''\n",
    "\thtml = urlopen(link)\n",
    "\tsoup = BeautifulSoup(html, 'lxml')\n",
    "\treturn soup.title.text\n",
    "\n",
    "def create_table_of_contents(dictionary, course_name, z_list):\n",
    "\t'''Receives course dictionary, course name, and list of unique z_numbers from get_whole_course().\n",
    "    Creates text file with contents of course, formatted in Markdown, with wiki-style links to each chapter.'''    \n",
    "\tfilename = z_list[-1] + ' ' + course_name + '.txt'\n",
    "\twith open(filename, 'a') as f:\n",
    "\t\tfor chapter, lessons in dictionary.items():\n",
    "\t\t\tz_index = list(dictionary.keys()).index(chapter)\n",
    "\t\t\tprint('\\n# ', '[[' + z_list[z_index] + ']]', chapter, '\\n', file=f)\n",
    "\t\t\tfor lesson_name, lesson_link in lessons:\n",
    "\t\t\t\tprint(\"   *\", lesson_name, file=f)\n",
    "\n",
    "def get_lesson_json(link):\n",
    "\t'''Receives lesson link from make_chapter_notes() and returns \n",
    "\tthe dictionary that holds all the information for the lesson's parent chapter.'''\n",
    "\thtml = urlopen(link)\n",
    "\tsoup = BeautifulSoup(html, 'lxml')\n",
    "\tstring = soup.find_all('script')[3].string\n",
    "\tjson_text = string.strip('window.PRELOADED_STATE=')[:-1]\n",
    "\tlesson_json = json.loads(json_text)\n",
    "\treturn lesson_json\n",
    "                \n",
    "def NormalExercise_print(json, f):\n",
    "\t'''Works with make_chapter_notes. Parses NormalExercise type lessons and prints them in \n",
    "\tmarkdown to file.\n",
    "\tFeeds to: convert_2_md(), get_success_msg().'''\n",
    "\twith open(f, 'a') as f:\n",
    "\t\tprint('#', json['title'], '\\n', file=f)\n",
    "\t\tprint('## Exercise\\n', file=f)\n",
    "\t\tprint(convert_2_md(json['assignment']), file=f)\n",
    "\t\tprint('## Instructions\\n', file=f)\n",
    "\t\tprint(convert_2_md(json['instructions'][:-2]), file=f)\n",
    "\t\tprint('## Code\\n', file=f)\n",
    "\t\tprint('```\\n' + convert_2_md(json['sample_code']).replace('\\\\', ''),'\\n```\\n', file=f)\n",
    "\t\tprint('```\\n' + convert_2_md(json['solution']).replace('\\\\', ''),'\\n```\\n', file=f)\n",
    "\t\tprint(get_success_msg(json['sct']) + '\\n', file=f)\n",
    "\n",
    "def BulletExercise_print(json, f):\n",
    "\t'''Works with make_chapter_notes. Parses BulletExercises type lessons and prints them in \n",
    "\tmarkdown to file.\n",
    "\tFeeds to: convert_2_md(), get_success_msg().'''\n",
    "\twith open(f, 'a') as f:\n",
    "\t\tprint('# ' + json['title'], '\\n', file=f)\n",
    "\t\tprint('## Exercise\\n', file=f)\n",
    "\t\tprint(convert_2_md(json['assignment']), file=f)\n",
    "\t\tprint('## Instructions & Code \\n', file=f)  \n",
    "\t\tfor item in json['subexercises']:\n",
    "\t\t\tprint(convert_2_md(item['instructions']), file=f)\n",
    "\t\t\tprint('```\\n' + item['sample_code'] + '\\n```\\n', file=f)\n",
    "\t\t\tprint('```\\n' + item['solution'] + '\\n```', file=f)\n",
    "\t\t\tprint(get_success_msg(item['sct']) + '\\n', file=f)\n",
    "\n",
    "def MultipleChoiceExercise_print(json, f):\n",
    "\t'''Works with make_chapter_notes. Parses MultipleChoice type lessons and prints them in \n",
    "\tmarkdown to file.\n",
    "\tFeeds to: convert_2_md(), get_correct_mc(), get_success_msg().'''\n",
    "\twith open(f, 'a') as f:\n",
    "\t\tprint('# ' + json['title'], '\\n', file=f)\n",
    "\t\tprint('## Exercise\\n', file=f)\n",
    "\t\tprint(convert_2_md(json['assignment']), file=f)\n",
    "\t\tprint(\"## Choices\\n\", file=f)\n",
    "\t\tfor choice in json['instructions']:\n",
    "\t\t\tprint('* ' + choice, file=f)\n",
    "\t\tprint('\\n**Correct answer: ' + get_correct_mc(json['sct']) + '**\\n', file=f)\n",
    "\t\tprint(get_success_msg(json['sct']) + '\\n', file=f)\n",
    "\n",
    "def convert_2_md(string): #Source: http://www.practicallyefficient.com/2016/12/04/pandoc-and-python.html\n",
    "\t'''Receives a string of HTML and use Pandoc to return string in Markdown.'''\n",
    "\tp = Popen(['pandoc', '-f', 'html', '-t', 'markdown', '--wrap=preserve'], stdout=PIPE, stdin=PIPE, stderr=STDOUT)\n",
    "\ttext = p.communicate(input=string.encode('utf-8'))[0]\n",
    "\ttext = unescape(text.decode('utf-8'))\n",
    "\treturn text\n",
    "\n",
    "def get_success_msg(string):\n",
    "\t'''Parses text from DataCamp `sct` JSON and returns the success message as a string.'''\n",
    "\tmatch = re.search(r'success_msg\\(\"(.*?)\"\\)', string)\n",
    "\tif match != None:\n",
    "\t\tmessage = match.group(1)\n",
    "\t\treturn message\n",
    "\telse:\n",
    "\t\treturn ''\n",
    "\n",
    "def get_correct_mc(string):\n",
    "\t'''Parses text from DataCamp `sct` JSON and correct answer for MultipleChoice type lessons. \n",
    "    Works with MultipleChoiceExercise_print'''\n",
    "\tmatch = re.search(r'test_mc\\(correct = (\\d),', string)\n",
    "\tif match != None:\n",
    "\t\tmessage = match.group(1)\n",
    "\t\treturn message\n",
    "\telse:\n",
    "\t\treturn ''\n",
    "\n",
    "def download_chapter_slides(chapter_names, one_link):\n",
    "\t'''Receives the list of chaper_names and a link to the first lesson of the first chapter\n",
    "\tfrom get_whole_course(). Gets links for the PDF slides for each chapter, downloads each PDF,\n",
    "\tand saves it with the same name as the chapter txt file for easy indexing.'''\n",
    "\tcourse_json = get_lesson_json(one_link)\n",
    "\tpdf_links = []\n",
    "\tfor item in course_json['course']['chapters']:\n",
    "\t\tpdf_links.append(item['slides_link'])\n",
    "\tpdf_tuples = list(zip(chapter_names, pdf_links))\n",
    "\tfor t in pdf_tuples:\n",
    "\t\tfilename = t[0].strip('.txt') + '.pdf'\n",
    "\t\tif t[1]:\n",
    "\t\t\turlretrieve(t[1], filename)\n",
    "\n",
    "def unescape(s): #Source: https://wiki.python.org/moin/EscapingHtml\n",
    "\t'''Receives string from convert_2_md() and unescapes non-ascii characters.'''\n",
    "\ts = s.replace(\"&lt;\", \"<\")\n",
    "\ts = s.replace(\"&gt;\", \">\")\n",
    "\ts = s.replace(\"&amp;\", \"&\")\n",
    "\treturn s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above code is loaded, all you have to do is grab the link for the course and run `get_whole_course()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "link = 'https://www.datacamp.com/courses/introduction-to-the-tidyverse'\n",
    "get_whole_course(link)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as a bonus, you can use the below function to scrape a whole track. In the next section, I demo what it looks like to scrape the track that I'm currently working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_whole_track(link):\n",
    "    html = urlopen(link)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    track = soup.find_all('a', attrs={'class':'course-block__link ds-snowplow-link-course-block'})\n",
    "    track_title = soup.title.text\n",
    "    courses = []\n",
    "\n",
    "    for x in track:\n",
    "        title = x.find('h4').text\n",
    "        tail = x.attrs['href']\n",
    "        url = 'https://www.datacamp.com' + tail\n",
    "        courses.append((title, url))\n",
    "    \n",
    "    track_file = '20180815170711 ' + track_title + '.txt'  \n",
    "    \n",
    "    with open(track_file, 'a') as f:\n",
    "        print('#', track_title + '\\n', file=f)\n",
    "        for course in courses:\n",
    "            z_num = get_whole_course(course[1], return_z=True)\n",
    "            course_name = '[[' + z_num + ']] ' + course[0]\n",
    "            print('*', course_name, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Action\n",
    "\n",
    "So what happens when you scrape a whole track? Here's the \"Data Scientist with R\" track, 23 courses that'll take you from R-zero to R-hero:\n",
    "\n",
    "![](/images/datacamp_track.png)\n",
    "\n",
    "Each course looks something like this:\n",
    "\n",
    "![](/images/datacamp_course.png)\n",
    "\n",
    "When I run the `get_whole_track()` function, here's the table of contents I end up with:\n",
    "\n",
    "![](/images/scrape_01.png)\n",
    "\n",
    "Here's the table of contents for the course:\n",
    "\n",
    "![](/images/scrape_02.png)\n",
    "\n",
    "Here's the first chapter of the course:\n",
    "\n",
    "![](/images/scrape_03.png)\n",
    "\n",
    "And that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "get_whole_track('https://www.datacamp.com/tracks/data-scientist-with-r')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
